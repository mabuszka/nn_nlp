{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNrFmdxZrAph"
      },
      "source": [
        "\n",
        "## Task 1 (5 points)\n",
        "\n",
        "Implement simplified word2vec with negative sampling from scratch (using pure numpy). Assume that in the training data objects and contexts are given explicitly, one pair per line, and objects are on the left. The result of the training should be object vectors. Please, write them to a file using *natural* text format, ie\n",
        "\n",
        "<pre>\n",
        "word1 x1_1 x1_2 ... x1_N \n",
        "word2 x2_1 x2_2 ... x2_N\n",
        "...\n",
        "wordK xK_1 xK_2 ... xk_N\n",
        "</pre>\n",
        "\n",
        "Use the loss from Slide 3 in Lecture NLP.2, compute the gradient manually. You can use some gradient clipping, or regularisation. \n",
        "\n",
        "**Remark**: the data is specially prepared to make the learning process easier. \n",
        "Present vectors using the code below. In this task we define success as 'obtaining a result which looks definitely not random'"
      ],
      "id": "lNrFmdxZrAph"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip6g5RnzrApl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "id": "ip6g5RnzrApl"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-53WILlFsjvr",
        "outputId": "1f8d397d-2d73-48da-d290-c4bd91b8ce0f"
      },
      "id": "-53WILlFsjvr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    sig = np.where(x < 0, np.exp(x)/(1 + np.exp(x)), 1/(1 + np.exp(-x)))\n",
        "    return sig"
      ],
      "metadata": {
        "id": "Xjp_6ru308-3"
      },
      "id": "Xjp_6ru308-3",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "U3Q0DPcorApo"
      },
      "outputs": [],
      "source": [
        "# words = pd.read_csv(\"task1_objects_contexts_polish.txt\", sep = \" \", names = [\"word_1\", \"word_2\"])\n",
        "# # unq_words = pd.Series(\n",
        "# pd.concat([words[\"word_1\"],words[\"word_2\"] ]).unique()\n",
        "# #     )\n",
        "\n",
        "\n",
        "class Word2Vec:\n",
        "    \n",
        "    def __init__(self, space_dim = 20):\n",
        "        self.space_dim = space_dim\n",
        "\n",
        "    \n",
        "    def upload_words(self, obj_cont_file):\n",
        "        words = pd.read_csv(obj_cont_file, sep = \" \", names = [\"word_1\", \"word_2\"])\n",
        "        self.context_freq = words.groupby(['word_2']).agg(\n",
        "            count=pd.NamedAgg(column=\"word_2\", aggfunc=\"count\")\n",
        "        )\n",
        "        self.objects = words[\"word_1\"].unique()\n",
        "        self.contexts = words[\"word_2\"].unique()\n",
        "        self.n_objects = len(self.objects)\n",
        "        self.n_contexts = len(self.contexts)\n",
        "        \n",
        "    def get_objects(self):\n",
        "        return(self.objects)\n",
        "    \n",
        "    def get_contexts(self):\n",
        "        return(self.contexts)\n",
        "    \n",
        "    def get_obj_id(self, obj):\n",
        "        return self.objects_to_id.loc[obj]\n",
        "    \n",
        "    def get_cont_id(self, cont):\n",
        "        return self.contexts_to_id.loc[cont]\n",
        "    \n",
        "#     def get_obj_id(self, obj):\n",
        "#         return self.objects_to_id.loc[obj]\n",
        "    \n",
        "    def make_maps(self):\n",
        "        self.contexts_to_id = pd.Series(np.arange(self.n_contexts), index = self.contexts)\n",
        "        self.id_to_contexts = pd.Series(self.contexts)\n",
        "        self.objects_to_id = pd.Series(np.arange(self.n_objects), index = self.objects)\n",
        "        self.id_to_objects = pd.Series(self.objects)\n",
        "        \n",
        "    def make_arrays(self):\n",
        "        rng = np.random.default_rng()\n",
        "        mu, sigma = 0, 0.1\n",
        "        self.objects_embed = rng.normal(size = (self.n_objects, self.space_dim))\n",
        "        self.contexts_embed = rng.normal(size = (self.n_contexts, self.space_dim))\n",
        "        \n",
        "    def calc_modified_unigram_probs(self):\n",
        "        self.id_context_freq = self.context_freq.set_index(self.contexts_to_id.loc[self.context_freq.index])\n",
        "        self.id_context_probs = self.id_context_freq.div(self.id_context_freq.sum(axis=0), axis=1)\n",
        "        self.id_context_probs = self.id_context_probs ** (3/4)\n",
        "        self.id_context_probs = self.id_context_freq.div(self.id_context_freq.sum(axis=0), axis=1)\n",
        "        self.id_context_probs.sort_index(inplace = True)\n",
        "        self.context_probs_array = np.squeeze(self.id_context_probs.to_numpy(copy = True))\n",
        "        \n",
        "    def generate_neg_samples(self, K):\n",
        "        rng = np.random.default_rng()\n",
        "        self.neg_samples = rng.choice(np.arange(0, self.n_contexts), size = (K, 1000000) , p = self.context_probs_array, replace = True)\n",
        "        self.neg_samples_pointer = 0   \n",
        "\n",
        "    def get_ready(self):\n",
        "        self.make_maps()\n",
        "        self.make_arrays()\n",
        "        self.calc_modified_unigram_probs()\n",
        "        self.neg_samples_pointer = None\n",
        "                   \n",
        "        \n",
        "    def get_neg_samples(self, word_idx, K):\n",
        "\n",
        "        # while True:\n",
        "        #     if self.neg_samples_pointer > 99999:\n",
        "        #         self.generate_neg_samples(K)\n",
        "        #     neg_samples_indices = self.neg_samples[:,self.neg_samples_pointer]\n",
        "        #     self.neg_samples_pointer += 1\n",
        "            \n",
        "        #     if  not np.any(np.isin(word_idx, neg_samples_indices)):\n",
        "        #         break\n",
        "\n",
        "        # while True:\n",
        "        if self.neg_samples_pointer > 999999:\n",
        "            self.generate_neg_samples(K)\n",
        "        neg_samples_indices = self.neg_samples[:,self.neg_samples_pointer]\n",
        "        self.neg_samples_pointer += 1\n",
        "            \n",
        "            # if  not np.any(np.isin(word_idx, neg_samples_indices)):\n",
        "                # break\n",
        "            \n",
        "        return neg_samples_indices\n",
        "\n",
        "\n",
        "        return neg_sample_indices\n",
        "    \n",
        "    def save_emmbedding(self, file_path):\n",
        "        self.id_to_objects.sort_index(inplace = True)\n",
        "        words = self.id_to_objects.to_numpy(copy = True)\n",
        "        words_embed = np.hstack((words[:,np.newaxis], self.objects_embed))\n",
        "#         print(words_embed)\n",
        "        np.savetxt(file_path, words_embed, delimiter=\" \", fmt ='%s', header = str(self.n_objects) + \" \" + str(self.space_dim), comments = \"\")\n",
        "    \n",
        "#     def calc_obj_fun(object_idx, context_idx, neg_contexts_indices):\n",
        "    \n",
        "    def train_word2vec(self, obj_cont_file_path, K = 3, lr = 0.1, reps = 1):\n",
        "\n",
        "        if self.neg_samples_pointer is None:\n",
        "            self.generate_neg_samples(K)\n",
        "\n",
        "        for i in range(reps):\n",
        "            print(\"Starting epoch \" + str(i) )\n",
        "\n",
        "            obj_cont_file = open(obj_cont_file_path, \"r\")\n",
        "            line_num = 0\n",
        "            for line in obj_cont_file:\n",
        "                line_num += 1\n",
        "                obj, cont = line.strip().split(\" \")\n",
        "                obj_idx = self.get_obj_id(obj)\n",
        "                cont_idx = self.get_cont_id(cont)\n",
        "                neg_cont_indices = self.get_neg_samples(cont_idx, K)\n",
        "\n",
        "                obj_vec = self.objects_embed[obj_idx,:][np.newaxis, :] # row vec\n",
        "                # print(\"obj\", obj_vec.shape)\n",
        "                cont_vec = self.contexts_embed[cont_idx,:][np.newaxis, :] # row vec\n",
        "                # print(\"cont\", cont_vec.shape)\n",
        "\n",
        "                neg_cont_vecs = self.contexts_embed[neg_cont_indices,:] # one neg cont is a row\n",
        "\n",
        "                # print(\"neg_cont\", neg_cont_vecs.shape)\n",
        "\n",
        "#                 obj_grad = obj_vec / (np.exp(-cont_vec.T @ obj_vec) + 1) # column vec\n",
        "#                 cont_grad = (cont_vec.T / (np.exp(-cont_vec.T @ obj_vec) -1) + \n",
        "#                             (neg_cont_vecs.T / (np.exp(neg_cont_vecs.T @ obj_vec) + 1)).sum(axis = 0, keepdims = True)) # column vec\n",
        "\n",
        "#                 neg_cont_grads = obj_vec / (np.exp(-neg_cont_vecs.T @ obj_vec) + 1).T # gradient for one u_k is one column\n",
        "                \n",
        "                cont_grad = (sigmoid(cont_vec @ obj_vec.T) - 1) * obj_vec # row vec            \n",
        "                obj_grad = (sigmoid(cont_vec @ obj_vec.T) - 1) * cont_vec + (sigmoid(neg_cont_vecs @ obj_vec.T) * neg_cont_vecs).sum(axis = 0, keepdims = True) # row vec\n",
        "                neg_cont_grads = sigmoid(neg_cont_vecs @ obj_vec.T) * neg_cont_vecs # row vec\n",
        "\n",
        "                # print(\"obj_grad\", obj_grad.shape)\n",
        "                # print(\"cont_grad\", cont_grad.shape)\n",
        "                # print(\"neg_cont_grads\", neg_cont_grads.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                self.objects_embed[obj_idx,:] -= np.squeeze(lr * obj_grad)\n",
        "                self.contexts_embed[cont_idx,:] -= np.squeeze(lr * cont_grad)\n",
        "\n",
        "                for grad_idx, neg_idx in enumerate(neg_cont_indices):\n",
        "                    # print(grad_idx, neg_idx)\n",
        "                    # print(neg_cont_grads[:,grad_idx])\n",
        "                    # print(lr)\n",
        "                    self.contexts_embed[neg_idx,:] -= np.squeeze(lr * neg_cont_grads[grad_idx,:])\n",
        "\n",
        "                if line_num % 100000 == 0:\n",
        "                    print(\"now on line \" + str(line_num))\n",
        "                    \n",
        "            obj_cont_file.close()\n",
        "\n",
        "            self.objects_embed /= self.objects_embed.max(axis = 0)\n",
        "            self.contexts_embed /= self.contexts_embed.max(axis = 0)\n",
        "\n",
        "            print(\"Done with epoch \" + str(i))\n",
        "        \n",
        "        \n",
        "\n",
        "        \n",
        "    \n",
        "    \n",
        "\n",
        "        \n"
      ],
      "id": "U3Q0DPcorApo"
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "jR4IznMmrApt"
      },
      "outputs": [],
      "source": [
        "w2v = Word2Vec(20)\n",
        "# w2v.upload_words(\"task1_objects_contexts_polish.txt\")\n",
        "w2v.upload_words(\"/content/drive/MyDrive/NN_NLP/task1_objects_contexts_polish.txt\")\n",
        "# context_freq = w2v.context_freq\n",
        "# w2v.make_maps()\n",
        "# # contexts_to_id = w2v.contexts_to_id\n",
        "# w2v.make_arrays()\n",
        "# id_context_freq = context_freq.set_index(contexts_to_id.loc[context_freq.index])\n",
        "# id_context_freq\n",
        "w2v.get_ready()\n",
        "# w2v.train_word2vec(\"task1_objects_contexts_polish.txt\")\n"
      ],
      "id": "jR4IznMmrApt"
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "1AQpbfLdrApu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d59373b4-7be5-4116-bc50-3a173d1b6784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 0\n",
            "now on line 100000\n",
            "now on line 200000\n",
            "now on line 300000\n",
            "now on line 400000\n",
            "now on line 500000\n",
            "now on line 600000\n",
            "now on line 700000\n",
            "now on line 800000\n",
            "now on line 900000\n",
            "now on line 1000000\n",
            "now on line 1100000\n",
            "now on line 1200000\n",
            "now on line 1300000\n",
            "now on line 1400000\n",
            "now on line 1500000\n",
            "now on line 1600000\n",
            "now on line 1700000\n",
            "now on line 1800000\n",
            "now on line 1900000\n",
            "now on line 2000000\n",
            "now on line 2100000\n",
            "now on line 2200000\n",
            "now on line 2300000\n",
            "now on line 2400000\n",
            "now on line 2500000\n",
            "now on line 2600000\n",
            "now on line 2700000\n",
            "now on line 2800000\n",
            "now on line 2900000\n",
            "now on line 3000000\n",
            "now on line 3100000\n",
            "now on line 3200000\n",
            "now on line 3300000\n",
            "now on line 3400000\n",
            "now on line 3500000\n",
            "now on line 3600000\n",
            "now on line 3700000\n",
            "now on line 3800000\n",
            "now on line 3900000\n",
            "now on line 4000000\n",
            "now on line 4100000\n",
            "now on line 4200000\n",
            "now on line 4300000\n",
            "now on line 4400000\n",
            "now on line 4500000\n",
            "now on line 4600000\n",
            "now on line 4700000\n",
            "now on line 4800000\n",
            "now on line 4900000\n",
            "now on line 5000000\n",
            "now on line 5100000\n",
            "now on line 5200000\n",
            "now on line 5300000\n",
            "now on line 5400000\n",
            "now on line 5500000\n",
            "Done with epoch 0\n"
          ]
        }
      ],
      "source": [
        "# w2v.train_word2vec(\"task1_objects_contexts_polish.txt\")\n",
        "w2v.train_word2vec(\"/content/drive/MyDrive/NN_NLP/task1_objects_contexts_polish.txt\")\n",
        "\n",
        "w2v.save_emmbedding(\"task1_w2v_vectors_2.txt\")"
      ],
      "id": "1AQpbfLdrApu"
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.train_word2vec(\"/content/drive/MyDrive/NN_NLP/task1_objects_contexts_polish.txt\")\n",
        "\n",
        "w2v.save_emmbedding(\"task1_w2v_vectors_3.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvxhl_joXiMC",
        "outputId": "5c0e5081-14bb-40eb-93d2-5349c3124c06"
      },
      "id": "cvxhl_joXiMC",
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 0\n",
            "now on line 100000\n",
            "now on line 200000\n",
            "now on line 300000\n",
            "now on line 400000\n",
            "now on line 500000\n",
            "now on line 600000\n",
            "now on line 700000\n",
            "now on line 800000\n",
            "now on line 900000\n",
            "now on line 1000000\n",
            "now on line 1100000\n",
            "now on line 1200000\n",
            "now on line 1300000\n",
            "now on line 1400000\n",
            "now on line 1500000\n",
            "now on line 1600000\n",
            "now on line 1700000\n",
            "now on line 1800000\n",
            "now on line 1900000\n",
            "now on line 2000000\n",
            "now on line 2100000\n",
            "now on line 2200000\n",
            "now on line 2300000\n",
            "now on line 2400000\n",
            "now on line 2500000\n",
            "now on line 2600000\n",
            "now on line 2700000\n",
            "now on line 2800000\n",
            "now on line 2900000\n",
            "now on line 3000000\n",
            "now on line 3100000\n",
            "now on line 3200000\n",
            "now on line 3300000\n",
            "now on line 3400000\n",
            "now on line 3500000\n",
            "now on line 3600000\n",
            "now on line 3700000\n",
            "now on line 3800000\n",
            "now on line 3900000\n",
            "now on line 4000000\n",
            "now on line 4100000\n",
            "now on line 4200000\n",
            "now on line 4300000\n",
            "now on line 4400000\n",
            "now on line 4500000\n",
            "now on line 4600000\n",
            "now on line 4700000\n",
            "now on line 4800000\n",
            "now on line 4900000\n",
            "now on line 5000000\n",
            "now on line 5100000\n",
            "now on line 5200000\n",
            "now on line 5300000\n",
            "now on line 5400000\n",
            "now on line 5500000\n",
            "Done with epoch 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.train_word2vec(\"/content/drive/MyDrive/NN_NLP/task1_objects_contexts_polish.txt\")\n",
        "\n",
        "w2v.save_emmbedding(\"task1_w2v_vectors_4.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFncPtov0kvT",
        "outputId": "d5a2348b-509d-438a-db21-25f29ee685db"
      },
      "id": "vFncPtov0kvT",
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 0\n",
            "now on line 100000\n",
            "now on line 200000\n",
            "now on line 300000\n",
            "now on line 400000\n",
            "now on line 500000\n",
            "now on line 600000\n",
            "now on line 700000\n",
            "now on line 800000\n",
            "now on line 900000\n",
            "now on line 1000000\n",
            "now on line 1100000\n",
            "now on line 1200000\n",
            "now on line 1300000\n",
            "now on line 1400000\n",
            "now on line 1500000\n",
            "now on line 1600000\n",
            "now on line 1700000\n",
            "now on line 1800000\n",
            "now on line 1900000\n",
            "now on line 2000000\n",
            "now on line 2100000\n",
            "now on line 2200000\n",
            "now on line 2300000\n",
            "now on line 2400000\n",
            "now on line 2500000\n",
            "now on line 2600000\n",
            "now on line 2700000\n",
            "now on line 2800000\n",
            "now on line 2900000\n",
            "now on line 3000000\n",
            "now on line 3100000\n",
            "now on line 3200000\n",
            "now on line 3300000\n",
            "now on line 3400000\n",
            "now on line 3500000\n",
            "now on line 3600000\n",
            "now on line 3700000\n",
            "now on line 3800000\n",
            "now on line 3900000\n",
            "now on line 4000000\n",
            "now on line 4100000\n",
            "now on line 4200000\n",
            "now on line 4300000\n",
            "now on line 4400000\n",
            "now on line 4500000\n",
            "now on line 4600000\n",
            "now on line 4700000\n",
            "now on line 4800000\n",
            "now on line 4900000\n",
            "now on line 5000000\n",
            "now on line 5100000\n",
            "now on line 5200000\n",
            "now on line 5300000\n",
            "now on line 5400000\n",
            "now on line 5500000\n",
            "Done with epoch 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.train_word2vec(\"/content/drive/MyDrive/NN_NLP/task1_objects_contexts_polish.txt\")\n",
        "\n",
        "w2v.save_emmbedding(\"task1_w2v_vectors_5.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC4HTvTusVpu",
        "outputId": "664c5bd5-ade5-4aaf-eca5-3c14971909e6"
      },
      "id": "RC4HTvTusVpu",
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 0\n",
            "now on line 100000\n",
            "now on line 200000\n",
            "now on line 300000\n",
            "now on line 400000\n",
            "now on line 500000\n",
            "now on line 600000\n",
            "now on line 700000\n",
            "now on line 800000\n",
            "now on line 900000\n",
            "now on line 1000000\n",
            "now on line 1100000\n",
            "now on line 1200000\n",
            "now on line 1300000\n",
            "now on line 1400000\n",
            "now on line 1500000\n",
            "now on line 1600000\n",
            "now on line 1700000\n",
            "now on line 1800000\n",
            "now on line 1900000\n",
            "now on line 2000000\n",
            "now on line 2100000\n",
            "now on line 2200000\n",
            "now on line 2300000\n",
            "now on line 2400000\n",
            "now on line 2500000\n",
            "now on line 2600000\n",
            "now on line 2700000\n",
            "now on line 2800000\n",
            "now on line 2900000\n",
            "now on line 3000000\n",
            "now on line 3100000\n",
            "now on line 3200000\n",
            "now on line 3300000\n",
            "now on line 3400000\n",
            "now on line 3500000\n",
            "now on line 3600000\n",
            "now on line 3700000\n",
            "now on line 3800000\n",
            "now on line 3900000\n",
            "now on line 4000000\n",
            "now on line 4100000\n",
            "now on line 4200000\n",
            "now on line 4300000\n",
            "now on line 4400000\n",
            "now on line 4500000\n",
            "now on line 4600000\n",
            "now on line 4700000\n",
            "now on line 4800000\n",
            "now on line 4900000\n",
            "now on line 5000000\n",
            "now on line 5100000\n",
            "now on line 5200000\n",
            "now on line 5300000\n",
            "now on line 5400000\n",
            "now on line 5500000\n",
            "Done with epoch 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "EYvAriwMrApw",
        "outputId": "486d78cc-e14f-43a9-db8e-91bb9283d983",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WORD: pies\n",
            "    dziewczyna 0.8752753734588623\n",
            "    chłopiec 0.8642098307609558\n",
            "    nadgarstek 0.8078317046165466\n",
            "    antywirus 0.7606443762779236\n",
            "    facet 0.757659912109375\n",
            "    kobieta 0.730688214302063\n",
            "    karawana 0.7299747467041016\n",
            "    arciszewski 0.7172316312789917\n",
            "    zwierzę 0.7157486081123352\n",
            "    przechodzień 0.7089031934738159\n",
            "\n",
            "WORD: kot\n",
            "    małpa 0.8216554522514343\n",
            "    kota 0.8076803684234619\n",
            "    mucha 0.8019682168960571\n",
            "    miś 0.796568751335144\n",
            "    gęś 0.7962770462036133\n",
            "    ocet 0.7945913672447205\n",
            "    chłopiec 0.7893929481506348\n",
            "    bez 0.7859345078468323\n",
            "    anka 0.7807961106300354\n",
            "    koza 0.7772140502929688\n",
            "\n",
            "WORD: miłość\n",
            "    wiara 0.8106540441513062\n",
            "    mitologia 0.7806751728057861\n",
            "    więź 0.7768008708953857\n",
            "    fantazja 0.7763660550117493\n",
            "    gynt 0.7680486440658569\n",
            "    cnota 0.7609277367591858\n",
            "    przyjaźń 0.7608277797698975\n",
            "    wolność 0.7574243545532227\n",
            "    wyobraźnia 0.7566429972648621\n",
            "    młodość 0.7522676587104797\n",
            "\n",
            "WORD: chłopiec\n",
            "    dziewczyna 0.932693362236023\n",
            "    pies 0.8642098307609558\n",
            "    kobieta 0.8405329585075378\n",
            "    chłopak 0.8392346501350403\n",
            "    łotr 0.8131989240646362\n",
            "    facet 0.8130990266799927\n",
            "    przechodzień 0.7959421873092651\n",
            "    chłopek 0.7943509221076965\n",
            "    mąż 0.7932397127151489\n",
            "    dziewczę 0.7923445105552673\n",
            "\n",
            "WORD: logika\n",
            "    motywacja 0.8737640380859375\n",
            "    cnota 0.8155385255813599\n",
            "    samoocena 0.8133038282394409\n",
            "    swoboda 0.8119058609008789\n",
            "    zależność 0.8068735599517822\n",
            "    spontaniczność 0.8060634136199951\n",
            "    cenzurowanie 0.8006764054298401\n",
            "    mentalność 0.7988549470901489\n",
            "    umiar 0.796645998954773\n",
            "    predyspozycja 0.7885147929191589\n",
            "\n",
            "WORD: ustawa\n",
            "    przepis 0.8659342527389526\n",
            "    rozwiązanie 0.857680082321167\n",
            "    umowa 0.84223473072052\n",
            "    obrusik 0.7916274070739746\n",
            "    zobowiązanie 0.7893222570419312\n",
            "    poprawka 0.7839500904083252\n",
            "    zapis 0.7771015763282776\n",
            "    dyrektywa 0.7729525566101074\n",
            "    norma 0.7596257925033569\n",
            "    postępowanie 0.7575379014015198\n",
            "\n",
            "WORD: kobieta\n",
            "    człowiek 0.9106030464172363\n",
            "    dziewczyna 0.8883437514305115\n",
            "    mężczyzna 0.8620681762695312\n",
            "    chłopiec 0.8405328989028931\n",
            "    osoba 0.8359000086784363\n",
            "    uczeń 0.8069561719894409\n",
            "    facet 0.801730751991272\n",
            "    mąż 0.7824171781539917\n",
            "    rodzic 0.7710436582565308\n",
            "    zwierzę 0.7451930046081543\n",
            "\n",
            "WORD: motyl\n",
            "    nóżka 0.8477239012718201\n",
            "    pająk 0.8286581039428711\n",
            "    kopyto 0.8259072303771973\n",
            "    ryczek 0.8244585394859314\n",
            "    kukła 0.8212769031524658\n",
            "    wełna 0.8176826238632202\n",
            "    sałata 0.815504789352417\n",
            "    trąbka 0.8089014887809753\n",
            "    warkocz 0.7976019978523254\n",
            "    ogórek 0.7965609431266785\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "# task1_wv = KeyedVectors.load_word2vec_format('task1_w2v_vectors.txt', binary=False)\n",
        "# task1_wv = KeyedVectors.load_word2vec_format('task1_w2v_vectors_2.txt', binary=False)\n",
        "# task1_wv = KeyedVectors.load_word2vec_format('task1_w2v_vectors_3.txt', binary=False)\n",
        "# task1_wv = KeyedVectors.load_word2vec_format('task1_w2v_vectors_4.txt', binary=False)\n",
        "task1_wv = KeyedVectors.load_word2vec_format('task1_w2v_vectors_5.txt', binary=False)\n",
        "\n",
        "\n",
        "\n",
        "example_english_words = ['dog', 'dragon', 'love', 'bicycle', 'marathon', 'logic', 'butterfly']  # replace, or add your own examples\n",
        "example_polish_words = ['pies', 'kot', 'miłość', 'chłopiec', 'logika', 'ustawa', 'kobieta', 'motyl']\n",
        "\n",
        "example_words = example_polish_words\n",
        "\n",
        "for w0 in example_words:\n",
        "    print ('WORD:', w0)\n",
        "    for w, v in task1_wv.most_similar(w0):\n",
        "        print ('   ', w, v)\n",
        "    print ()"
      ],
      "id": "EYvAriwMrApw"
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "0qWti7eurApx"
      },
      "outputs": [],
      "source": [
        "w2v.save_emmbedding(\"task1_w2v_vectors_4.txt\")"
      ],
      "id": "0qWti7eurApx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_82Oiy6rApy",
        "outputId": "5c28bdd3-e9cd-4ae7-af10-b0a26f9c5231"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nagromadzenie G2_następstwo\n",
            "\n"
          ]
        }
      ],
      "source": [
        "obj_cont_file = open(\"task1_objects_contexts_polish.txt\", \"r\")\n",
        "for line in obj_cont_file:\n",
        "    obj, cont = line.split(\" \")\n",
        "    print(obj, cont)\n",
        "    break\n",
        "obj_cont_file.close()"
      ],
      "id": "q_82Oiy6rApy"
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "s99Z4zy-rApz",
        "outputId": "ba5e5672-03c5-4158-d255-1c55a4fc7a7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "100 % 10"
      ],
      "id": "s99Z4zy-rApz"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "assignment4_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}